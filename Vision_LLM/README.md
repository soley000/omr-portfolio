# Vision + LLM (Ã‰valuation hallucinations)

## ğŸ“Œ Contexte
Projet IA appliquÃ©e pour Ã©valuer les hallucinations dâ€™un modÃ¨le multimodal Vision + LLM.
Objectif : fiabiliser les rÃ©ponses gÃ©nÃ©rÃ©es et analyser les limites du modÃ¨le.

## ğŸ§© ProblÃ©matique
Les LLM multimodaux peuvent produire des hallucinations ou erreurs factuelles.
DÃ©tecter et mesurer ces hallucinations est essentiel pour lâ€™usage en production.

## âš™ï¸ Pipeline / Architecture
```mermaid
graph LR
A[Images d'entrÃ©e] --> B[PrÃ©traitement]
B --> C[ModÃ¨le Recognize Anything + LLM]
C --> D[Sortie texte annotÃ©]
D --> E[Analyse hallucinations]
````
ğŸ›  MÃ©thodologie

PrÃ©traitement des images (normalisation, redimensionnement)

GÃ©nÃ©ration de lÃ©gendes / rÃ©ponses avec LLM

Ã‰valuation des hallucinations avec mÃ©triques automatisÃ©es (precision, recall)

Comparaison entre modÃ¨les et rÃ©glage hyperparamÃ¨tres

ğŸ“Š Dataset

Mini dataset simulÃ© : 5 images fictives + descriptions CSV

Colonnes : image_id, description_attendue

ğŸ† RÃ©sultats / Livrables

Tableau de mÃ©triques par image et modÃ¨le

Graphiques : hallucinations les plus frÃ©quentes

Impact : meilleure comprÃ©hension des limites du modÃ¨le

âš ï¸ Limites / Perspectives

Extension Ã  dâ€™autres types dâ€™images

Automatisation de la correction des hallucinations

ğŸ’¡ Recommandations / Next Steps

IntÃ©grer module de validation humaine
